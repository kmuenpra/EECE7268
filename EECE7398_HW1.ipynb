{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmuenpra/EECE7268/blob/main/EECE7398_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EECE 7398: Verifiable Machine Learning -- Fall 2025\n",
        "# Exercise #1: Adversarial Examples & Decision Boundaries\n",
        "\n",
        "**Submission Instructions:** Please upload a .ipynb file (including your code, plots, and written answers) to Gradescope. This file can be updated unlimited times until the submission deadline."
      ],
      "metadata": {
        "id": "vd517Au4POB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Train & Attack an MNIST Classifier"
      ],
      "metadata": {
        "id": "EXD6SNWSGkec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1a) Train an MNIST Classifier\n",
        "\n",
        "To start, train a NN classifier that consists of:\n",
        "- a \"Flatten\" layer to convert the (28, 28) images to (784,)\n",
        "- a fully connected layer with 128 neurons and ReLU activation\n",
        "- a fully connected (output) layer with 10 neurons (10 neurons for 10 digit categories)\n",
        "- (optionally) a softmax layer to convert logits to probabilities\n",
        "\n",
        "You can use the Adam optimizer with cross entropy loss (make sure to use the correct loss depending on whether you included a softmax layer) and it should work pretty well.\n",
        "\n",
        "**Deliverables**:\n",
        "- Plot your train and validation loss curves as a function of epoch\n",
        "- From a test set, provide some examples where your classifier predicted correctly & incorrectly (if applicable)"
      ],
      "metadata": {
        "id": "6UWtfBl0GxdM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P89tKVeVHlIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1b) Attack your MNIST classifier with constant offsets\n",
        "\n",
        "Add a constant perturbation to your test set images (e.g., c*torch.ones(28, 28)) and evaluate the performance as the magnitude increases. Also, be sure to clip your perturbed images so each pixel remains in [0, 1] (or [0, 255]).\n",
        "\n",
        "**Deliverables**:\n",
        "- Plot your test accuracy as a function of the perturbation magnitude\n",
        "- Provide some examples where the un-perturbed image was labeled correctly and the perturbed image was labeled incorrectly, despite the perturbed image looking reasonable to a human."
      ],
      "metadata": {
        "id": "VQHs1xx-IsIz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMJUADNfKs1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1c) Attack your MNIST classifier with noise\n",
        "\n",
        "Add some noise perturbations to your test set images and evaluate the performance as the noise magnitude increases. Also, be sure to clip your perturbed images so each pixel remains in [0, 1] (or [0, 255]).\n",
        "\n",
        "**Deliverables**:\n",
        "- Plot your test accuracy as a function of the perturbation magnitude\n",
        "- A couple sentences about any differences observed with constant perturbations vs. noise perturbations"
      ],
      "metadata": {
        "id": "eNP3rw47Ktun"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwArzDBELUx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1d) Attack your MNIST classifier with FGSM\n",
        "\n",
        "Implement the Fast Gradient Sign Method (FGSM) attack algorithm and experiment with different values of $\\epsilon$. You should write FGSM \"from scratch\" (e.g., using pytorch/numpy to evaluate the gradient, not using a library/method that already implements FGSM).\n",
        "\n",
        "**Deliverables**:\n",
        "- Plot your test accuracy as a function of the perturbation magnitude\n",
        "- A couple sentences about how this curve compares to the constant perturbations and noise perturbations (hopefully, FGSM should be able to achieve lower test accuracy for the same attack magnitude as those model-agnostic attacks)"
      ],
      "metadata": {
        "id": "jW8vSFeYLWLP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbAp7X1gLVj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1e) Attack your MNIST classifier with Targeted FGSM\n",
        "\n",
        "Implement the Targeted Fast Gradient Sign Method (TFGSM) attack algorithm and experiment with different values of $\\epsilon$. You should write TFGSM \"from scratch\" (e.g., using pytorch/numpy to evaluate the gradient, not using a library/method that already implements TFGSM).\n",
        "\n",
        "**Deliverables**:\n",
        "- Plot your test accuracy as a function of the perturbation magnitude for each target class\n",
        "- A few example images where you were able to successfully cause the classifier to predict the targeted (incorrect) class\n",
        "- A few sentences about your model's susceptibility to targeted attacks (e.g., are some target classes \"easier\" to get your model to classify? if you try to perturb images labeled 0 to a target class of 8, does the image get perturbed in an intuitive way?)"
      ],
      "metadata": {
        "id": "Pl47otM2MbyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HamVq9OBN5vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Investigating Decision Boundaries\n",
        "\n",
        "In this problem, you will train a binary classifier for a 2D input vector (is the (x, y) coordinate inside a circle of radius r centered at the origin?).\n",
        "\n",
        "Since you will know the true decision boundary that the model is trained to approximate, you can generate training data.\n",
        "\n",
        "Then, you will investigate the model's decision boundary to see how well it matches the true decision boundary."
      ],
      "metadata": {
        "id": "Pt0lzkwCN908"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2a) Generate training data\n",
        "\n",
        "For $x\\in R^2$, using the decision rule $f(x) = \\begin{cases} 1, &\\lvert\\lvert x \\rvert\\rvert_2 \\leq 3 \\\\ 0, & \\text{o.w.} \\end{cases}$, generate a training, validation, and test set of $[x, f(x)]$ pairs.\n",
        "\n",
        "**Deliverables:**\n",
        "- Plot your training set and the true decision boundary"
      ],
      "metadata": {
        "id": "O9h4ladNPGiW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u7442Jr-PHmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2b) Train NN classifier\n",
        "\n",
        "Train a NN classifier on the data you just generated.\n",
        "\n",
        "**Deliverables**:\n",
        "- Plot your train & validation loss curves as a function of epoch"
      ],
      "metadata": {
        "id": "hiuRYyAMkjzP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBJF_ueXk7tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2c) Compare decision boundaries\n",
        "\n",
        "It is unlikely that your model has learned the true decision boundary perfectly. Here, you will investigate the differences.\n",
        "\n",
        "**Deliverables**:\n",
        "- Plot the true and learned decision boundaries\n",
        "- A few sentences (with relevant figures to illustrate observations) about any differences and how changes to the model architecture or training procedure can impact decision boundaries (e.g., adding regularization, changing the training data generation method, changing the activation functions)."
      ],
      "metadata": {
        "id": "b8jUO2wMk8B0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6IczGKCemiP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Attack Competition on a Pre-Trained CIFAR-10 Classifier [Extra Credit]"
      ],
      "metadata": {
        "id": "2fdFT_qTm_EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 White-Box $l_\\infty$-norm Attack\n",
        "\n",
        "Here, you will have access to the full model (i.e., you know all of its parameters and architecture, you can compute gradients). You should write a function that takes in:\n",
        "- the model\n",
        "- a set of images + corresponding labels\n",
        "- a perturbation magnitude,\n",
        "\n",
        "and returns a perturbed version of the provided images that will cause the model to have poor classification accuracy.\n",
        "\n",
        "We provide a template for your attack function and a function for checking the classification accuracy so that you can prototype locally. Once you upload your .ipynb to Gradescope (make sure it is named `hw1.ipynb`), we will run your attack function against some random test images and maintain a leaderboard of the strongest attack (we provide the actual evaluation script below for your convenience). Every student that gets the accuracy below 40% will get some extra credit, and the student with strongest attack will get more extra credit. In case of a tie, we will look at runtime. You can keep updating your submission until the deadline."
      ],
      "metadata": {
        "id": "puAqsTTyniAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time"
      ],
      "metadata": {
        "id": "VS7GJ2MbBxqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a suggested way of getting the CIFAR-10 dataset:"
      ],
      "metadata": {
        "id": "thgnqpbO5EgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "batch_size = 256\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                         pin_memory=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "Dpm14TZyCKtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the model you'll be attacking (ResNet20):"
      ],
      "metadata": {
        "id": "CLyJ-z-_480o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)"
      ],
      "metadata": {
        "id": "l7rQk6sFpiSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While CIFAR-10 images have pixel values $\\in [0, 1]$, the model we're attacking was trained on a normalized version of CIFAR-10 (i.e., each image was shifted/scaled by a mean/std). You can attack the model in the original pixel coordinates (that's the space we'll enforce the attack magnitude), but remember that any time you want to query the model, you should first normalize the images according to those settings:"
      ],
      "metadata": {
        "id": "b_IAvWVr53Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(imgs):\n",
        "  mean = (0.4914, 0.4822, 0.4465)\n",
        "  std = (0.2023, 0.1994, 0.2010)\n",
        "  Normalize = transforms.Normalize(mean=mean, std=std)\n",
        "  return Normalize(imgs)"
      ],
      "metadata": {
        "id": "LkUWwI0r52er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function may help you when familiarizing yourself with the dataset and baseline model performance (and shows an example of using the normalize function right before querying the model):"
      ],
      "metadata": {
        "id": "MVZ3SNBX5V7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quickly_run_model(model, testloader, num_to_show=4):\n",
        "  dataiter = iter(testloader)\n",
        "\n",
        "  # `images` coming out of the dataloader are tensors of size (batch, 3, 32, 32)\n",
        "  # where each pixel is within [0, 1].\n",
        "  images, labels = next(dataiter)\n",
        "\n",
        "  model = model.to(device)\n",
        "  images = images.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  # The model was trained on a normalized version of the dataset\n",
        "  # (i.e., each pixel was scaled/shifted by mean and std).\n",
        "  # ==> we will normalize our images in that same way before querying model\n",
        "  normalized_images = normalize(images)\n",
        "  outputs = model(normalized_images)\n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "  print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                                for j in range(num_to_show)))\n",
        "  print('Labels: ', ' '.join(f'{classes[labels[j]]:5s}'\n",
        "                                for j in range(num_to_show)))\n",
        "\n",
        "quickly_run_model(model, testloader)"
      ],
      "metadata": {
        "id": "Dk_yRrgnB9xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function you will fill in is `white_box_attack`. The autograder expects this exact function name and signature, so make sure those stay the same:"
      ],
      "metadata": {
        "id": "TM2tstnL5djz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def white_box_attack(model, raw_images, labels, attack_magnitude):\n",
        "\n",
        "  # TODO: Fill in this method with your attack algorithm\n",
        "  # For example, a non-attack would be:\n",
        "  perturbed_images = raw_images\n",
        "\n",
        "  return perturbed_images"
      ],
      "metadata": {
        "id": "c8zb8VFbp399"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To help you debug, here's the function we will use to evaluate your `white_box_attack`. This should give 92.6% accuracy when there is no attack. This function is meant to be able to run standalone (i.e., not in a Jupyter notebook where the model/dataset may already be loaded), so feel free to modify it to be more efficient when debugging. We will run your attack against our own replica of this function on Gradescope's servers, so the runtime numbers will likely differ from what you observe locally or on Colab:"
      ],
      "metadata": {
        "id": "zoiOY-iD7Lue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_attacked_accuracy(attack_magnitude, attack_fn=white_box_attack, device='cpu'):\n",
        "\n",
        "  # We will run your attack against this function to establish the leaderboard\n",
        "\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          pin_memory=True)\n",
        "\n",
        "  model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
        "\n",
        "  attack_runtime = 0.\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  for raw_images, labels in testloader:\n",
        "\n",
        "    raw_images = raw_images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    attack_runtime_start = time.time()\n",
        "    normalized_raw_images = Normalize(raw_images)\n",
        "    perturbed_images = attack_fn(model, raw_images, normalized_raw_images, labels, attack_magnitude)\n",
        "    attack_runtime_end = time.time()\n",
        "    attack_runtime += attack_runtime_end - attack_runtime_start\n",
        "\n",
        "    assert torch.all(perturbed_images <= 1.) and torch.all(perturbed_images >= 0.), \"Perturbed images contain pixels outside of [0, 1].\"\n",
        "    assert torch.all(torch.linalg.vector_norm(perturbed_images - raw_images, ord=torch.inf, dim=(1,2,3)) <= attack_magnitude + 1e-6), \"Perturbation violates attack magnitude limit.\"\n",
        "\n",
        "    # calculate outputs by running images through the network\n",
        "    with torch.no_grad():\n",
        "      normalized_perturbed_images = normalize(perturbed_images)\n",
        "      outputs = model(normalized_perturbed_images)\n",
        "\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "\n",
        "  return accuracy, attack_runtime"
      ],
      "metadata": {
        "id": "U21xemd27Edv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run the above evaluation function as follows:"
      ],
      "metadata": {
        "id": "dYhwdAPO7eGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will enforce that the l_inf norm of the perturbation is <= attack_magnitude\n",
        "attack_magnitude = 1./255\n",
        "accuracy, attack_runtime = evaluate_attacked_accuracy(attack_magnitude)\n",
        "print(f\"Accuracy: {robust_accuracy} %\")\n",
        "print(f\"Attack Runtime: {attack_runtime} [s]\")"
      ],
      "metadata": {
        "id": "YZMbgxgQ7G8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}