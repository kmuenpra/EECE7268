{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmuenpra/EECE7268/blob/main/EECE7268_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EECE/CS 7268: Verifiable Machine Learning -- Fall 2025\n",
        "# HW#2: Formal Verification\n",
        "\n",
        "**Submission Instructions:** Please upload a .ipynb file (including your code, plots, and written answers) to Gradescope. This file can be updated unlimited times until the submission deadline.\n",
        "\n",
        "The goal of this assignment is to give you hands-on experience with formal verification of neural networks. You will code some verification algorithms from scratch in the first few problems to build an understanding of how these concepts from class actually play out in code. Then, you will get to use some libraries to gain familiarity with state-of-the-art software in this research field.\n",
        "\n",
        "You're welcome to use PyTorch, TensorFlow, Keras, or other appropriate libraries to help make your life easier throughout the problems - we tried to make the function signatures somewhat framework-agnostic."
      ],
      "metadata": {
        "id": "d8n5f4t_BW7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Please provide your collaboration statement here or we will assign a zero on this assignment**:\n"
      ],
      "metadata": {
        "id": "78VPN7GaPzoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment, we'll use the [NNet](https://github.com/sisl/nnet) format as a language/framework-agnostic description of a ReLU network and its parameters. Here are some possibly useful functions that convert between nnet, numpy arrays, and pytorch models (feel free to use these or not)."
      ],
      "metadata": {
        "id": "7GCrrocr3-SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "1PkaMYBAIQXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nnet_to_weights_and_biases(nnet_filename: str) -> tuple[np.ndarray, np.ndarray]:\n",
        "  # load a nnet text file and extract the weights and biases\n",
        "\n",
        "  # converted this code from: https://github.com/sisl/NeuralVerification.jl/blob/957cb32081f37de57d84d7f0813f708288b56271/src/utils/util.jl#L10\n",
        "  with open(nnet_filename, 'r') as f:\n",
        "    line = f.readline()\n",
        "    while \"//\" in line: #skip comments\n",
        "      line = f.readline()\n",
        "    # number of layers\n",
        "    nlayers = int(line.strip().split(\",\")[0])\n",
        "    # read in layer sizes\n",
        "    layer_sizes = [int(x) for x in f.readline().split(\",\")[1:nlayers+1]]\n",
        "    # read past additonal information\n",
        "    for i in range(1, 6):\n",
        "      line = f.readline()\n",
        "    # i=1 corresponds to the input dimension, so it's ignored\n",
        "    Ws = []\n",
        "    bs = []\n",
        "    for dim in layer_sizes:\n",
        "      W = np.vstack([[float(x) for x in f.readline().rstrip(',\\n ').split(\",\")] for i in range(dim)])\n",
        "      b = np.array([float(f.readline().rstrip(',\\n ')) for _ in range(dim)])\n",
        "      Ws.append(W)\n",
        "      bs.append(b)\n",
        "\n",
        "  return Ws, bs\n",
        "\n",
        "def weights_and_biases_to_pytorch(Ws: list[np.ndarray], bs: list[np.ndarray]) -> torch.nn.Sequential:\n",
        "  # given weights and biases, create a torch.nn.Sequential model with relus\n",
        "  # btwn each linear layer (except no activation on final layer)\n",
        "\n",
        "  num_layers = len(Ws)\n",
        "\n",
        "  layers = []\n",
        "  for W, b in zip(Ws, bs):\n",
        "    out_features, in_features = W.shape\n",
        "    layer = torch.nn.Linear(in_features, out_features)\n",
        "    layer.weight = torch.nn.Parameter(torch.Tensor(W))\n",
        "    layer.bias = torch.nn.Parameter(torch.Tensor(b))\n",
        "    layers.append(layer)\n",
        "    layers.append(torch.nn.ReLU())\n",
        "\n",
        "  # don't add ReLU to end of NN\n",
        "  model = torch.nn.Sequential(*layers[:-1])\n",
        "  return model\n",
        "\n",
        "def nnet_to_pytorch(nnet_filename: str) -> torch.nn.Sequential:\n",
        "  # directly convert from nnet text file to pytorch model\n",
        "  Ws, bs = nnet_to_weights_and_biases(nnet_filename)\n",
        "  model = weights_and_biases_to_pytorch(Ws, bs)\n",
        "  return model\n",
        "\n",
        "def model_to_weights_and_biases(model: torch.nn.Sequential) -> tuple[np.ndarray, np.ndarray]:\n",
        "  # extract the weights and biases as numpy arrays from a torch.nn.Sequential model\n",
        "  Ws = []\n",
        "  bs = []\n",
        "  for idx, m in enumerate(model.modules()):\n",
        "    if isinstance(m, torch.nn.Sequential):\n",
        "      continue\n",
        "    elif isinstance(m, torch.nn.ReLU):\n",
        "      continue\n",
        "    elif isinstance(m, torch.nn.Linear):\n",
        "      Ws.append(m.weight.data.numpy())\n",
        "      bs.append(m.bias.data.numpy())\n",
        "    else:\n",
        "      print(\"That layer isn't supported.\")\n",
        "      assert 0\n",
        "  return Ws, bs"
      ],
      "metadata": {
        "id": "aq31nltZE4it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nominal_and_epsilon_to_range(nominal: np.ndarray, epsilon: np.ndarray | float) -> np.ndarray:\n",
        "  return np.vstack([nominal-epsilon, nominal+epsilon]).T\n",
        "\n",
        "def range_to_nominal_and_epsilon(input_range: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "  nominal_input = (input_range[:, 1] + input_range[:, 0]) / 2.\n",
        "  epsilon = (input_range[:, 1] - input_range[:, 0]) / 2.\n",
        "  return nominal_input, epsilon"
      ],
      "metadata": {
        "id": "-k9pOOVv6CU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, to load a NN controller for a cartpole system, then query the controller for its action at a particular state:"
      ],
      "metadata": {
        "id": "wBUJmk-s5Qd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cartpole Controller\")\n",
        "\n",
        "# Load the NNet file and pass a nominal input to get the class logits\n",
        "nnet_filename = \"cartpole_nnet.nnet\"\n",
        "nominal_input = torch.Tensor([[0., 0.1, 0.2, 0.3]]) # these 4 values correspond to the 4 states in cartpole\n",
        "model = nnet_to_pytorch(nnet_filename)\n",
        "Ws, bs = model_to_weights_and_biases(model)\n",
        "nominal_output = model(nominal_input)\n",
        "print(f\"{nominal_output=}\")\n",
        "\n",
        "# Also set up an example set of possible inputs to this NN\n",
        "epsilon = 0.1 # epsilon can be a scalar or a vec w/ same shape as nominal_input\n",
        "input_range = nominal_and_epsilon_to_range(nominal_input, epsilon)\n",
        "print(f\"{input_range=}\")"
      ],
      "metadata": {
        "id": "uSVI1MUj5Pk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Sampling-Based Under-Approximations"
      ],
      "metadata": {
        "id": "KzPeANq0V5UW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to approximate the set of possible NN outputs for a given set of possible NN inputs is by exhaustively sampling. Here, you will implement a simple sampling-based method for estimating the min and max values for each dimension of the NN output. This sampling-based strategy provides an under-approximation of the output set (and the corresponding rectangular bound is an under-approximation of the axis-aligned bounding box around the true output set).\n",
        "\n",
        "The cartpole NN only has 4 inputs and 2 outputs, so you can visualize its output samples easily.\n",
        "\n",
        "**Deliverables**:\n",
        "\n",
        "- Implement the `output_range_sampling` function\n",
        "- For the cartpole NN, plot your output samples and the corresponding rectangular bounds for a few different values of `num_samples`\n"
      ],
      "metadata": {
        "id": "a2nSdrKB2OOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_range_sampling(nnet_filename: str, input_range: np.ndarray, num_samples: int) -> np.ndarray:\n",
        "\n",
        "  # TODO: implement this!\n",
        "\n",
        "  return output_range\n",
        "\n",
        "num_samples = int(1e4)\n",
        "output_range_sampling(nnet_filename, input_range, num_samples)"
      ],
      "metadata": {
        "id": "LCmrgL-cV_C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Interval Bound Propagation (IBP)"
      ],
      "metadata": {
        "id": "wMbeEqg6BlMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you will implement the IBP algorithm to get *outer* bounds on the output set -- which are much more useful for proving that the NN will not produce undesired outputs. You may want to read [the IBP paper](https://arxiv.org/pdf/1810.12715.pdf) (especially around Eqn 6) to help with this.\n",
        "\n",
        "**Deliverables:**\n",
        "\n",
        "- Implement the `verify_ibp` function\n",
        "- Print the `output_range` from your IBP implementation\n",
        "- Plot your IBP rectangular bounds along with the samples from above to see how ridiculously loose IBP bounds can be"
      ],
      "metadata": {
        "id": "lTuoeXsjAURM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_ibp(nnet_filename: str, input_range: np.ndarray) -> np.ndarray:\n",
        "\n",
        "  # TODO: implement this!\n",
        "\n",
        "  return output_range\n",
        "\n",
        "output_range_ibp = verify_ibp(nnet_filename, input_range)\n",
        "print(f\"{output_range_ibp=}\")"
      ],
      "metadata": {
        "id": "0UaowPAfD2ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Linear Program (LP)"
      ],
      "metadata": {
        "id": "1lNkJ9q7FN02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since IBP's bounds are usually too loose to say something useful about the NN's output set, you will now implement an LP-based verification algorithm that should provide much tighter bounds while remaining computationally reasonable.\n",
        "\n",
        "You will set up an LP (e.g., using `cvxpy` to set up variables, constraints, and objectives corresponding to the NN verification problem) for each side of the rectangle (i.e., min & max per output dimension, which can be done by simply changing the objective). To turn the ReLU constraints into linear equality/inequality constraints, you can check which neurons are \"active\", \"inactive\", or \"uncertain\" and add the corresponding relaxed constraints. To know which neurons belong to which category, you will need to acquire intermediate bounds -- there are lots of ways to do this!"
      ],
      "metadata": {
        "id": "VMPPb9KjAZfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_lp(nnet_filename: str, input_range: np.ndarray) -> np.ndarray:\n",
        "\n",
        "  # TODO: implement this!\n",
        "\n",
        "  return output_range\n",
        "\n",
        "output_range_lp = verify_lp(nnet_filename, input_range)\n",
        "print(f\"{output_range_lp=}\")"
      ],
      "metadata": {
        "id": "XsMaoa6aFXIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4: Verify Classifier Robustness"
      ],
      "metadata": {
        "id": "LKmmrX0Guiep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, you have written verification algorithms that provide upper and lower bounds on each output of the NN. Here, you will slightly modify your LP verifier so that it verifies the *robustness* of a *classifier* (i.e., prove that the classifier predicts the same class for every input within some range).\n",
        "\n",
        "If the verification algorithm is able to prove robustness, you should return `VerifierResults.Robust`. If the verification algorithm is unable to prove robustness, you should return `VerifierResults.Unsure` (remember that this doesn't necessarily mean the classifier isn't robust, just that our algorithm failed to find a proof). I also added a `VerifierResults.Timeout` option, because you will often see this in real verification algorithms, but you do not need to use this here.\n",
        "\n",
        "To start, you can pretend the cartpole controller was a classifier and try to see how large of an epsilon you can specify until you can't verify the robustness anymore.\n",
        "\n",
        "Then, you can load an MNIST classifier (code below) and try to see how large of an epsilon for which you can still verify robustness around a nominal image. Since the MNIST classifier is a way bigger NN, there will be some impact on the runtime of your LP.\n",
        "\n",
        "**Deliverables:**\n",
        "\n",
        "- Implement `verify_classification_robustness_lp`\n",
        "- Plot and/or briefly describe your findings w.r.t. robustness verification as you increase epsilon for the cartpole controller\n",
        "- Plot and/or briefly describe your findings w.r.t. robustness verification as you increase epsilon for the MNIST classifier"
      ],
      "metadata": {
        "id": "20SXabwqAfEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VerifierResults(Enum):\n",
        "  Robust = 0\n",
        "  Unsure = 1\n",
        "  Timeout = 2\n",
        "\n",
        "def verify_classification_robustness_lp(nnet_filename: str, input_range: np.ndarray) -> int:\n",
        "\n",
        "  # TODO: implement this!\n",
        "\n",
        "  return VerifierResults.Robust\n",
        "\n",
        "verifier_result = verify_classification_robustness_lp(nnet_filename, input_range)"
      ],
      "metadata": {
        "id": "y_vSMbXcvYWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a NN classifier for MNIST, then query the model at a particular image:"
      ],
      "metadata": {
        "id": "OmTWXVJ65a1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MNIST Classifier\")\n",
        "\n",
        "# Load the NNet file and pass a nominal input to get the class logits\n",
        "nnet_filename_mnist = \"mnist1.nnet\"\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "img_index = 2 # this one should be correctly classified as a 4...\n",
        "nominal_input_mnist = torch.Tensor(x_train[img_index].flatten())\n",
        "model_mnist = nnet_to_pytorch(nnet_filename_mnist)\n",
        "Ws_mnist, bs_mnist = model_to_weights_and_biases(model_mnist)\n",
        "nominal_output = model_mnist(nominal_input_mnist)\n",
        "nominal_class = nominal_output.argmax()\n",
        "print(f\"{nominal_output=}\")\n",
        "print(f\"Predicted Class: {nominal_class}, True Class: {y_train[img_index]}\")\n",
        "\n",
        "# Also set up an example set of possible inputs to this NN\n",
        "\n",
        "epsilon_mnist = 2*np.ones((784))\n",
        "input_range_mnist = nominal_and_epsilon_to_range(nominal_input_mnist, epsilon_mnist)\n",
        "print(f\"{input_range_mnist=}\")"
      ],
      "metadata": {
        "id": "LlJYRiLv5fdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verifier_result = verify_classification_robustness_lp(nnet_filename_mnist, input_range_mnist)"
      ],
      "metadata": {
        "id": "h5Z8Wsr-6HiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5: Explore the `jax_verify` library for verification"
      ],
      "metadata": {
        "id": "O6LR1io6Bpht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few excellent open-source libraries for NN verification. For example, `jax_verify` has implementations of IBP, CROWN, and many other algorithms. These methods can handle a broader class of NNs than just the ReLU NNs you worked with above, so they may be useful tools to leverage in your research or projects.\n",
        "\n",
        "A big upside of `jax_verify` is that it supports JIT compilation, which can speed up verification algorithms by 10-100x (you don't need to set up JIT compilation for this assignment, but you're encouraged to try that out if you're curious).\n",
        "\n",
        "Note that the `jax_verify` that is available by default in PyPi is an old version from 2020 that has a slightly different API - installing a more recent version is recommended as we do below."
      ],
      "metadata": {
        "id": "RWh1SWEEAkND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install my fork of jax_verify, which fixes a few deprecation errors in the mainline repo\n",
        "!pip install git+https://gitlab.com/neu-autonomy/certifiable-learning/jax_verify@mainline_repo"
      ],
      "metadata": {
        "id": "7ic1irPeBiZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax_verify\n",
        "import functools"
      ],
      "metadata": {
        "id": "XisKYJtrp1JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some utility methods that help set up the NN in jax:"
      ],
      "metadata": {
        "id": "NbNHanmT3il0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pytorch_model_to_jax(torch_model: torch.nn.Sequential):\n",
        "  params = []\n",
        "  act = None\n",
        "\n",
        "  # Extract params (weights, biases) from torch layers, to be used in\n",
        "  # jax.\n",
        "  # Note: This propagator assumes a feed-forward relu NN.\n",
        "  for m in torch_model.modules():\n",
        "    if isinstance(m, torch.nn.Sequential):\n",
        "      continue\n",
        "    elif isinstance(m, torch.nn.ReLU):\n",
        "      if act is None or act == \"relu\":\n",
        "        act = \"relu\"\n",
        "    elif isinstance(m, torch.nn.Linear):\n",
        "      w = m.weight.data.numpy().T\n",
        "      b = m.bias.data.numpy()\n",
        "      params.append((w, b))\n",
        "  return functools.partial(relu_nn, params)\n",
        "\n",
        "def relu_nn(params, inputs):\n",
        "  for W, b in params[:-1]:\n",
        "    outputs = jnp.dot(inputs, W) + b\n",
        "    inputs = jnp.maximum(outputs, 0)\n",
        "  W, b = params[-1]\n",
        "  return jnp.dot(inputs, W) + b\n",
        "\n",
        "def jax_interval_to_np_range(interval: jax_verify.IntervalBound) -> np.ndarray:\n",
        "  return np.vstack([interval.lower, interval.upper]).T\n",
        "\n",
        "def np_range_to_jax_interval(input_range: np.ndarray) -> jax_verify.IntervalBound:\n",
        "  return jax_verify.IntervalBound(input_range[:, 0], input_range[:, 1])\n",
        "\n",
        "jax_model = pytorch_model_to_jax(model)"
      ],
      "metadata": {
        "id": "UAg7ZvB5o-vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of using jax_verify's implementation of IBP on our cartpole control NN"
      ],
      "metadata": {
        "id": "dhi8nfG53v8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a fwd pass on the NN using jax\n",
        "nominal_output_jax = jax_model(jnp.array(nominal_input))\n",
        "\n",
        "# Example of computing bounds using IBP as implemented by jax_verify\n",
        "input_bounds = np_range_to_jax_interval(input_range)\n",
        "output_bounds_ibp_jax = jax_verify.interval_bound_propagation(\n",
        "    jax_model, input_bounds)\n",
        "output_range_ibp_jax = jax_interval_to_np_range(output_bounds_ibp_jax)\n",
        "print(f\"output bounds via IBP (jax_verify): \\n{output_range_ibp_jax}\")"
      ],
      "metadata": {
        "id": "V2YWoAyD3pur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does that output match the output of your IBP implementation?"
      ],
      "metadata": {
        "id": "Q6bl9cwNyHOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for your job: take a look at jax_verify's documentation to see how to use the backward CROWN verifier.\n",
        "\n",
        "**Deliverables:**\n",
        "\n",
        "- output bounds using backward CROWN, rather than IBP"
      ],
      "metadata": {
        "id": "dkY2_OOayXJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYsSp-XC0gcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 6: Explore the `auto-LiRPA` library for verification"
      ],
      "metadata": {
        "id": "y7SInFa3zGmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`auto-LiRPA` is a verification library based on PyTorch and one of the main developers was a Northeastern PhD student (Kaidi Xu)!\n",
        "\n",
        "Just like with the previous problem, the next cells show you how to install the package and run IBP, then we ask you to look into their documentation and run backward CROWN on the same problem.\n",
        "\n",
        "**Note:** I suggest doing this part last, because pip may try to downgrade the version of pytorch installed to satisfy requirements. You may also need to restart your kernel after the pip install. If you're on Colab and things seem messed up, you can always do `Runtime > Disconnect and delete runtime` (stronger than restarting the runtime, which doesn't reset to the default set of installed packages)."
      ],
      "metadata": {
        "id": "1Y6E6KwYAoNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-lirpa"
      ],
      "metadata": {
        "id": "87SnKjZJ0i9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of using jax_verify's implementation of IBP on our cartpole control NN:"
      ],
      "metadata": {
        "id": "G0C_PPb96xiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
        "\n",
        "# Wrap the model with auto_LiRPA.\n",
        "model = BoundedModule(model, torch.Tensor(nominal_input))\n",
        "# Define perturbation. Here we add Linf perturbation to input data.\n",
        "ptb = PerturbationLpNorm(norm=np.inf, eps=torch.Tensor([epsilon]))\n",
        "# Make the input a BoundedTensor with the pre-defined perturbation.\n",
        "my_input = BoundedTensor(torch.Tensor(nominal_input), ptb)\n",
        "# Regular forward propagation using BoundedTensor works as usual.\n",
        "prediction = model(my_input)\n",
        "# Compute LiRPA bounds using IBP\n",
        "lb, ub = model.compute_bounds(x=(my_input,), method=\"ibp\")\n",
        "output_range_ibp_autolirpa = np.vstack([lb.detach().numpy(), ub.detach().numpy()]).T\n",
        "print(f\"output bounds via IBP (auto-lirpa): \\n{output_range_ibp_autolirpa}\")"
      ],
      "metadata": {
        "id": "CYx4P1PG0kd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does that match your implementation of IBP? What about `jax_verify`'s?"
      ],
      "metadata": {
        "id": "K_N0KiGy64dW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for your job: compute output bounds using `auto-LiRPA`'s implementation of backward CROWN\n",
        "\n",
        "**Deliverables:**\n",
        "\n",
        "- output bounds using backward CROWN, rather than IBP"
      ],
      "metadata": {
        "id": "ZWHwMApQ1_FN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oAosulP31SiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All done!"
      ],
      "metadata": {
        "id": "nmKg8pm15U-l"
      }
    }
  ]
}